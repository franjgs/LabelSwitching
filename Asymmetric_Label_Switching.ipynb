{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Asymmetric_Label_Switching.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOcUUzreDFRJrmYGybW3PCP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franjgs/LabelSwitching/blob/main/Asymmetric_Label_Switching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYqCrl5xxtVc"
      },
      "source": [
        "# Label Switching\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j1f6txiyHre"
      },
      "source": [
        "## Ensembles\n",
        "\n",
        "Machine ensembles allow to improve the performance of single learning\n",
        "machines by aggregating the outputs of a number of them -learning units or\n",
        "learners- that are trained under diverse conditions [1, 2]. Since the 1990s,\n",
        "they served, in particular, to alleviate the practical limitations that the size\n",
        "of training datasets impose to the approximation capacity of shallow Multi-\n",
        "Layer Perceptrons (MLPs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJOGrTKNzU0x"
      },
      "source": [
        "### Diversity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvQryZTj16mo"
      },
      "source": [
        "#### Bagging\n",
        "\n",
        "Bagging type diversifications (resampling) introduce some risks {deletion of critical samples, emphasis of outliers, and even sample generation can create distortions in the class likelihoods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG3yppwN2K3t"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiNJXmbEzfII"
      },
      "source": [
        "#### Label Switching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD9c2OgEzXqA"
      },
      "source": [
        "## Imbalanced Problems\n",
        "\n",
        "Imbalanced classi\fcation deserves much attention because it appears in\n",
        "many important real world problems of di\u000berent application areas. Those\n",
        "problems have much di\u000berent class population sizes and/or classi\fcations\n",
        "costs.\n",
        "As a consequence, conventionally designed discriminative classi\fers do\n",
        "not provide satisfactory results: They naturally tend to decide in favour of\n",
        "the majority class. Among the procedures that serve to combat this undesirable\n",
        "e\u000bect {references [10, 11, 12, 13, 14, 15] o\u000ber clear and complete\n",
        "perspectives,{ there are methods based on sample preprocessing, and other\n",
        "that apply modi\fed learning algorithms, such as one-class or modi\fed kernel\n",
        "Support Vector Machines [16, 17]. Fuzzy formulations have also been applied\n",
        "[18]. Recently, there are contributions that include time-sensitive elements\n",
        "[19, 20]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCU0j8Xf0q6z"
      },
      "source": [
        "### Techniques for Imbalanced Problems\n",
        "The techniques based on preprocessing the (training) samples are simple\n",
        "and effective. \n",
        "\n",
        "They include, \n",
        "\n",
        "1. Increasing the cost of wrongly classifying the minority samples [21]\n",
        "2. Oversampling the minority and/or undersampling the majority classes [22, 23]{that can be considered as asymmetric versions of bagging,\n",
        "3. Generating minority samples {SMOTE [24] is a successful mechanism from which many modi\fcations have been proposed.\n",
        "\n",
        "Obviously, the classification problems that result from these processes are more balanced, and, under certain conditions, the solution of the original classification problem can be obtained from their results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVI0vA2n0R6H"
      },
      "source": [
        "###Asymmetric Label Switching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eRoMcBazlMs"
      },
      "source": [
        "# Code for Assymetric Label Switching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3FD_Y4i8g4N"
      },
      "source": [
        "# Load some standard packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybQORDb2xr4o"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import time\n",
        "import scipy.io as sio\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhoNHTHZJJsA"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score, roc_curve, auc, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def perf_measure(y_actual, y_hat):\n",
        "\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "\n",
        "    for i in range(len(y_hat)):\n",
        "        if y_actual[i]==y_hat[i]:\n",
        "            if y_actual[i]==1:\n",
        "                TP += 1\n",
        "            else:\n",
        "                TN += 1\n",
        "        else:\n",
        "            if y_actual[i]==1:\n",
        "                FN += 1\n",
        "            else:\n",
        "                FP += 1\n",
        "\n",
        "    return TP, FP, TN, FN\n",
        "\n",
        "def ROC_points(L, f):\n",
        "\n",
        "  \"\"\"\n",
        "  Calculates the value of the discriminant function for a dx1 dimensional\n",
        "  sample given covariance matrix and mean vector.\n",
        "  \"\"\"\n",
        "\n",
        "  FPR=[]\n",
        "  TPR=[]\n",
        "  P=len(np.where(L==+1)[0])\n",
        "  N=len(np.where(L==-1)[0])\n",
        "\n",
        "  B=sorted(range(len(f)),key=lambda x:f[x],reverse=True)\n",
        "\n",
        "  L_sorted=L[B]\n",
        "  FP=0\n",
        "  TP=0\n",
        "  f_prev=1e45\n",
        "  i=0\n",
        "\n",
        "  while i < len(L_sorted):\n",
        "      if f[i] !=f_prev:\n",
        "          FPR.append(FP/float(N))\n",
        "          TPR.append(TP/float(P))\n",
        "          f_prev=f[i]\n",
        "      if L_sorted[i]==+1:\n",
        "          TP+=1\n",
        "      else:\n",
        "          FP+=1\n",
        "      i+=1\n",
        "\n",
        "  FPR.append(FP/float(N))\n",
        "  TPR.append(TP/float(P))\n",
        "\n",
        "  return FPR, TPR"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4QY8hMf9Rx0"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "\n",
        "nh = 4\n",
        "# MLP0 = MLPRegressor(solver='lbfgs', alpha = 1, activation = 'tanh', hidden_layer_sizes=(nh, ), max_iter=1000) #, random_state=1)\n",
        "MLPc = MLPClassifier(solver='lbfgs', alpha = 1, activation = 'tanh', hidden_layer_sizes=(nh, ), max_iter=1000) #, random_state=1)\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),                                # 0. Accepatble performance \n",
        "    SVC(kernel=\"linear\", C=0.025, probability=True),        # 1. Very Bad performance. Very Slow (5-fold validation to estimate pyX)\n",
        "    SVC(gamma=2, C=1, probability=True),                    # 2. Very Bad performance. Very Slow (5-fold validation to estimate pyX)\n",
        "    GaussianProcessClassifier(1.0 * RBF(1.0)),              # 3. Very very Slow\n",
        "    DecisionTreeClassifier(max_depth=5),                    # 4. Accepatble performance \n",
        "    RandomForestClassifier(max_depth=5, n_estimators=3),    # 5. Bad Performace  max_features=1),\n",
        "    GradientBoostingClassifier(n_estimators = 3),           # 6. Very Bad performance\n",
        "    AdaBoostClassifier(n_estimators = 3, base_estimator = GaussianNB(), learning_rate=1),   # 7. Bad performance\n",
        "    GaussianNB(),                                           # 8. Medium Performace. Very Fast\n",
        "    QuadraticDiscriminantAnalysis(),                        # 9.  Accepatble performance. Very fast. Fails at execution\n",
        "    MLPc                                                    # 10. Best Performace. Slow convergence.\n",
        "    ]\n",
        "\n",
        "K_ens= [1,3] #, 101]\n",
        "learner_ens = classifiers[0]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoYqy9t4-cJO"
      },
      "source": [
        "Label Switching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dccewfoi-fRp"
      },
      "source": [
        "Conventional (Symmetric) Switching "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXnQv8TX9fOu"
      },
      "source": [
        "def breiman_switching(y, frsw=0.0):\n",
        "\n",
        "    # frsw: Label Switching Rate from Minority to Majority class\n",
        "\n",
        "    ysw=1*y;\n",
        "\n",
        "    idx1=np.where(y==+1)[0]\n",
        "    idx0=np.where(y==-1)[0]\n",
        "\n",
        "    n_flipped=int(round(len(idx1)*frsw))\n",
        "\n",
        "    idx1_sw=np.random.choice(idx1, n_flipped, replace=False)\n",
        "    ysw[idx1_sw]=-y[idx1_sw]\n",
        "\n",
        "    idx0_sw=np.random.choice(idx0, n_flipped, replace=False)\n",
        "    ysw[idx0_sw]=-y[idx0_sw]\n",
        "\n",
        "    return ysw"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6_vlz-_-lVK"
      },
      "source": [
        "Asymmetric Label Switching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov3hrLqM9uZD"
      },
      "source": [
        "def label_switching(y, alphasw=0.0, betasw=0.0):\n",
        "\n",
        "    # alphasw: Label Switching Rate from Majority to Minority class\n",
        "    # betasw: Label Switching Rate from Minority to Majority class\n",
        "\n",
        "    ysw=1*y;\n",
        "\n",
        "    idx1=np.where(y==+1)[0]\n",
        "    l1=len(idx1)\n",
        "    bet_1=int(round(l1*betasw))\n",
        "    idx1_sw=np.random.choice(idx1,bet_1, replace=False)\n",
        "    ysw[idx1_sw]=-y[idx1_sw]\n",
        "\n",
        "    idx0=np.where(y==-1)[0]\n",
        "    l0=len(idx0)\n",
        "    alph_0=int(round(l0*alphasw))\n",
        "    idx0_sw=np.random.choice(idx0,alph_0, replace=False)\n",
        "    ysw[idx0_sw]=-y[idx0_sw]\n",
        "\n",
        "    return ysw"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHAXAnj9-r7J"
      },
      "source": [
        "Load UCI Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "156P6ndeKojO"
      },
      "source": [
        "Load the dataset via its URL and check the following:\n",
        "\n",
        "- Are the headers showing up properly?\n",
        "- Look at the first 5 and the last 5 rows, do they seem to be in order?\n",
        "- Does the dataset have the correct number of rows and columns as described in the UCI page? \n",
        " - Remember, that UCI does not count the y variable (column of values that we might want to predict via a machine learning model) as an \"attribute\" but rather as a \"class attribute\" so you may end up seeing a number of columns that is one greater than the number listed on the UCI website.\n",
        "- Does UCI list this dataset as having missing values? Check for missing values and see if your analysis corroborates what UCI reports?\n",
        "- if `NaN` values or other missing value indicators are not being detected by `df.isnull().sum()` find a way to replace whatever is indicating the missing values with `np.NaN`.\n",
        "- Use the .describe() function in order to see the summary statistics of both the numeric and non-numeric columns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKVsp1lD_Z6v"
      },
      "source": [
        "# Load data from URL using pandas read_csv method\n",
        "filename_i='balance-scale'\n",
        "URL_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'+filename_i+'/'+filename_i+'.data'\n",
        "# print(URL_path)\n",
        "Xy = pd.read_csv(URL_path)\n",
        "di = {'L': -1, 'B': +1, 'R': -1}\n",
        "Xy.replace({'B': di}, inplace=True)\n",
        "y = np.array(Xy['B'])\n",
        "X = np.array(Xy.drop(['B'], axis=1))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxh0rcSfg99D"
      },
      "source": [
        "filename_i='wine-quality'\n",
        "\n",
        "URL_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'+filename_i+'/winequality-red.csv'\n",
        "# print(URL_path)\n",
        "Xy = pd.read_csv(URL_path,  sep=';')\n",
        "\n",
        "Xy['quality'] = Xy['quality'].apply(lambda x: +1 if x <= 4 else -1)\n",
        "y = np.array(Xy['quality'])\n",
        "X = np.array(Xy.drop(['quality'], axis=1))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbtAhP5HB995",
        "outputId": "cbefa11f-d8db-48b8-b25f-e5aba58d2718"
      },
      "source": [
        "dim = X.shape[1]\n",
        "\n",
        "N0 = len(np.where(y==-1)[0])\n",
        "N1 = len(np.where(y==1)[0])\n",
        "\n",
        "index0 = np.where(y==-1)[0]\n",
        "index1 = np.where(y==1)[0]\n",
        "\n",
        "print('N0: %d. N1: %d' % (N0 , N1))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N0: 1536. N1: 63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT8p0gEoEs2R"
      },
      "source": [
        "######## Costes y Probabilidades ########\n",
        "P0=float(N0)/float(N0+N1)\n",
        "P1=float(N1)/float(N0+N1)\n",
        "QP=P0/P1\n",
        "\n",
        "P0_1=0.5 # Para la balanced accuracy\n",
        "P1_1=0.5\n",
        "\n",
        "C10=1 # False Positive (FP)\n",
        "C00=0 # True Negative (TN)\n",
        "C01=1 # False Negative (FN)   : QC=1/5\n",
        "C11=0 # True Positive (TP)\n",
        "\n",
        "QC=float(C10-C00)/float(C01-C11)\n",
        "QCT=QC\n",
        "\n",
        "Q=QC*QP\n",
        "######## Costes y Probabilidades ########"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqTn-3yXEr0v"
      },
      "source": [
        "Common Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeZr5kUcFH7y"
      },
      "source": [
        "###################### COMMON PARAMETERS ######################\n",
        "\n",
        "n_folds = 1\n",
        "Nruns   = 20\n",
        "Test_size = 25\n",
        "\n",
        "index0 = np.where(y==-1)[0]\n",
        "index1 = np.where(y==1)[0]\n",
        "\n",
        "l_test0 = int(N0*0.01*Test_size)\n",
        "l_test1 = int(N1*0.01*Test_size)\n",
        "\n",
        "Alpha = np.linspace(0,0.45,10) # Label Switching Rate from Majority to Minority class\n",
        "Alpha = [0, 0.1, 0.2, 0.3, 0.4]\n",
        "Beta = np.linspace(0,0.2,5)     # Label Switching Rate from Minority to Majority class\n",
        "Beta = [0]\n",
        "eps=np.finfo(float).eps"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsLgusFHGaYP"
      },
      "source": [
        "filename_o=filename_i+'_NW_nh_'+str(nh)+'_TS_'+str(Test_size)+'.mat'\n",
        "\n",
        "perf_sw_K1 = np.zeros([Nruns, len(Alpha), len(Beta), len(K_ens), 6]) # Confusion Matrix + MCC + F1\n",
        "perf_sw_K2 = np.zeros([Nruns, len(Alpha), len(Beta), len(K_ens)])   # CBL\n",
        "perf_sw_K3 = np.zeros([Nruns, len(Alpha), len(Beta), len(K_ens)])   # AuC\n",
        "\n",
        "lROC=1001\n",
        "mean_fpr = np.logspace(-4, 0, lROC)\n",
        "perf_sw_K4=np.zeros([len(Alpha), len(Beta), len(K_ens), lROC, 2])     # Test:  ROC(FPR, TPR)\n",
        " "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG3sCHobHNa5"
      },
      "source": [
        "Main Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "ad2IG12lHP3I",
        "outputId": "9fe25731-dc9e-45df-e2dc-1bfdff0d667c"
      },
      "source": [
        "b_idx=0\n",
        "for beta in Beta:\n",
        "    a_idx=0\n",
        "    for alpha in Alpha:        \n",
        "        start = time.time()\n",
        "        for nruns in range(Nruns):\n",
        "            \n",
        "            idx0_test = np.random.choice(index0, l_test0, replace=False)\n",
        "            idx1_test = np.random.choice(index1, l_test1, replace=False)\n",
        "            idx_test = np.sort(np.concatenate((idx0_test, idx1_test), axis=0))\n",
        "\n",
        "            X_train = np.delete(X, idx_test, axis=0)\n",
        "            X_test = X[idx_test]\n",
        "            y_train = np.delete(y, idx_test, axis=0)\n",
        "            y_test = y[idx_test]\n",
        "                         \n",
        "            N0_tr = len(np.where(y_train==-1)[0])\n",
        "            N1_tr = len(np.where(y_train==1)[0])\n",
        "\n",
        "            P0_tr = float(N0_tr)/float(N0_tr+N1_tr)\n",
        "            P1_tr = float(N1_tr)/float(N0_tr+N1_tr)\n",
        "            \n",
        "            QP_tr = P0_tr/P1_tr\n",
        "\n",
        "            y_true = y_test\n",
        "\n",
        "            k_idx = 0\n",
        "            for K in K_ens:\n",
        "\n",
        "                o_x=np.ones([len(y_test)])\n",
        "\n",
        "                o_x_tst_ens=np.zeros([K,len(y_test)])\n",
        "\n",
        "                model_name=model_name = type(learner_ens).__name__                \n",
        "                MLP_ens = [learner_ens for kkk in range(K)]\n",
        "\n",
        "                for k in range(K):              \n",
        "                    #################### NORM ####################\n",
        "                    scaler = StandardScaler()\n",
        "                    scaler.fit(X_train)\n",
        "                    X_train_n = scaler.transform(X_train)\n",
        "                    X_test_n = scaler.transform(X_test)\n",
        "                    #################### NORM ####################\n",
        "                    \n",
        "                    ################## Switching ##################\n",
        "                    if alpha==0 and beta==0:\n",
        "                        X_train_s, y_train_s = X_train_n, y_train\n",
        "                    else:\n",
        "                        X_train_s = X_train_n\n",
        "                        y_train_s = label_switching(y_train, alpha, beta)\n",
        "                    \n",
        "                    \n",
        "                    N0_sw = len(np.where(y_train_s==-1)[0])\n",
        "                    N1_sw = len(np.where(y_train_s==1)[0])\n",
        "\n",
        "                    P0_sw = float(N0_sw)/float(N0_sw+N1_sw)\n",
        "                    P1_sw = float(N1_sw)/float(N0_sw+N1_sw)\n",
        "\n",
        "                    QP_sw = P0_sw/P1_sw\n",
        "                    ################## Switching #################\n",
        "                    MLP_ens[k] = MLP_ens[k].fit(X_train_s, y_train_s)\n",
        " \n",
        "                    y_pred = MLP_ens[k].predict_proba(X_test_n)[:,1]*2-1\n",
        "                    \n",
        "                    o_x_tst_ens[k] = y_pred\n",
        "                    \n",
        "                o_x_tst=o_x_tst_ens.mean(0)\n",
        "                pyX_test=(0.5*(o_x_tst+1)-alpha)/(1-alpha-beta)\n",
        "                \n",
        "                FPR_vec, TPR_vec, thresholds = roc_curve(y_true, pyX_test)\n",
        "                roc_auc = auc(FPR_vec, TPR_vec)\n",
        "                perf_sw_K3[nruns,a_idx,b_idx, k_idx] = roc_auc\n",
        "\n",
        "                f_ROC = interp1d(FPR_vec, TPR_vec)\n",
        "\n",
        "                perf_sw_K4[a_idx,b_idx,k_idx,:,0] += mean_fpr\n",
        "                perf_sw_K4[a_idx,b_idx,k_idx,:,1] += f_ROC(mean_fpr)\n",
        "\n",
        "                gamma = (2 * (alpha + (1 - alpha - beta) * (QCT / (QCT + 1))) - 1)\n",
        " \n",
        "                o_x2=np.ones([len(y_test)])\n",
        "                o_x2[o_x_tst < gamma] = -1\n",
        "\n",
        "                TP, FP, TN, FN = perf_measure(y_true, o_x2) \n",
        "\n",
        "                # Sensitivity, hit rate, recall, or true positive rate\n",
        "                TPR=TP/float(TP+FN)\n",
        "\n",
        "                # Specificity (SPC) or true negative rate\n",
        "                SPC=TN/float(TN+FP)\n",
        "\n",
        "                # False positive rate: FPR=FP/float(FP+TN) = 1-SPC\n",
        "                FPR=1-SPC\n",
        "\n",
        "                # False negative rate\n",
        "                FNR=1-TPR\n",
        "\n",
        "                # Mathhews correlation coefficient\n",
        "                if (((TP + FP) == 0) or ((TP + FN) == 0) or ((TN + FP) == 0) or ((TN + FN) == 0)):\n",
        "                    MCC = 0\n",
        "                else:\n",
        "                    MCC = (TP * TN - FP * FN) / np.sqrt(float(TP + FP) * float(TP + FN) * float(TN + FP) * float(TN + FN))\n",
        "\n",
        "                # Accuracy\n",
        "                ACC=(TP+TN)/float(TP+FN+FP+TN)\n",
        "\n",
        "                # F1 Score\n",
        "                F1=2*TP/float(2*TP+FP+FN)\n",
        "\n",
        "                perf_sw_K1[nruns,a_idx,b_idx,k_idx,0]=FPR\n",
        "                perf_sw_K1[nruns,a_idx,b_idx,k_idx,1]=TPR\n",
        "                perf_sw_K1[nruns,a_idx,b_idx,k_idx,2]=SPC\n",
        "                perf_sw_K1[nruns,a_idx,b_idx,k_idx,3]=ACC\n",
        "                perf_sw_K1[nruns,a_idx,b_idx,k_idx,4]=MCC\n",
        "                perf_sw_K1[nruns,a_idx,b_idx,k_idx,5]=F1\n",
        "\n",
        "                perf_sw_K2[nruns,a_idx,b_idx,k_idx]=(FPR*P0_1+FNR*P1_1) # Balanced Accuracy\n",
        "\n",
        "                k_idx+=1\n",
        "\n",
        "        end = time.time()\n",
        "        print('TRAIN TIME:')\n",
        "        print('%.2gs'%(end-start))\n",
        "        print('Alpha: %.2f. Beta: %.2f. IR_sw: %.2f.'  %(alpha,beta,QP_sw))\n",
        "\n",
        "        \n",
        "        for k_idx in range(len(K_ens)):\n",
        "            print('MLP: %d. F1: %f. +- %f MCC: %f. +- %f TPR: %f. FPR: %f' %(K_ens[k_idx],\n",
        "                                                                             np.mean(perf_sw_K1[:,a_idx,b_idx,k_idx,5]), \n",
        "                                                                             np.std(perf_sw_K1[:,a_idx,b_idx,k_idx,5]), \n",
        "                                                                             np.mean(perf_sw_K1[:,a_idx,b_idx,k_idx,4]), \n",
        "                                                                             np.std(perf_sw_K1[:,a_idx,b_idx,k_idx,4]), \n",
        "                                                                             np.mean(perf_sw_K1[:,a_idx,b_idx,k_idx,1]), \n",
        "                                                                             np.mean(perf_sw_K1[:,a_idx,b_idx,k_idx,0])))\n",
        "        a_idx+=1\n",
        "    b_idx+=1\n",
        "\n",
        "perf_sw_K1_std = np.std(perf_sw_K1, axis=0)\n",
        "perf_sw_K1 = np.mean(perf_sw_K1, axis=0)\n",
        "\n",
        "perf_sw_K2_std = np.std(perf_sw_K2, axis=0)\n",
        "perf_sw_K2 = np.mean(perf_sw_K2, axis=0)\n",
        "\n",
        "perf_sw_K3_std = np.std(perf_sw_K3, axis=0)\n",
        "perf_sw_K3 = np.mean(perf_sw_K3, axis=0)\n",
        "\n",
        "perf_sw_K4 /= n_folds*Nruns\n",
        "\n",
        "tprPoints = perf_sw_K4[:,0,1,:,0]\n",
        "fprPoints = perf_sw_K4[:,0,1,:,1]\n",
        "plt.plot(tprPoints[0,:],fprPoints[0,:], label='Alpha 00')\n",
        "plt.plot(tprPoints[1,:],fprPoints[1,:], label='Alpha 01')\n",
        "plt.plot(tprPoints[2,:],fprPoints[2,:], label='Alpha 02')\n",
        "plt.plot(tprPoints[3,:],fprPoints[3,:], label='Alpha 03')\n",
        "plt.plot(tprPoints[4,:],fprPoints[4,:], label='Alpha 04')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "sio.savemat(model_name+'_'+filename_o, {'perf_sw_K1': perf_sw_K1,  \n",
        "                                        'perf_sw_K2': perf_sw_K2, \n",
        "                                        'perf_sw_K3': perf_sw_K3, \n",
        "                                        'perf_sw_K1_std': perf_sw_K1_std,  \n",
        "                                        'perf_sw_K2_std': perf_sw_K2_std, \n",
        "                                        'perf_sw_K3_std': perf_sw_K3_std, \n",
        "                                        'perf_sw_K4': perf_sw_K4, \n",
        "                                        'Alpha':Alpha, 'Beta': Beta, 'K_ens': K_ens})\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN TIME:\n",
            "1.1s\n",
            "Alpha: 0.00. Beta: 0.00. IR_sw: 24.00.\n",
            "MLP: 1. F1: 0.117163. +- 0.068022 MCC: 0.132591. +- 0.086353 TPR: 0.076667. FPR: 0.007812\n",
            "MLP: 3. F1: 0.117163. +- 0.068022 MCC: 0.132591. +- 0.086353 TPR: 0.076667. FPR: 0.007812\n",
            "TRAIN TIME:\n",
            "1.1s\n",
            "Alpha: 0.10. Beta: 0.00. IR_sw: 6.36.\n",
            "MLP: 1. F1: 0.131643. +- 0.083230 MCC: 0.096339. +- 0.085387 TPR: 0.150000. FPR: 0.041146\n",
            "MLP: 3. F1: 0.152553. +- 0.091785 MCC: 0.138349. +- 0.097735 TPR: 0.120000. FPR: 0.016016\n",
            "TRAIN TIME:\n",
            "1.1s\n",
            "Alpha: 0.20. Beta: 0.00. IR_sw: 3.32.\n",
            "MLP: 1. F1: 0.127858. +- 0.044085 MCC: 0.090105. +- 0.055081 TPR: 0.273333. FPR: 0.119010\n",
            "MLP: 3. F1: 0.142565. +- 0.071600 MCC: 0.131176. +- 0.080762 TPR: 0.106667. FPR: 0.014974\n",
            "TRAIN TIME:\n",
            "1.1s\n",
            "Alpha: 0.30. Beta: 0.00. IR_sw: 2.05.\n",
            "MLP: 1. F1: 0.097961. +- 0.037528 MCC: 0.056762. +- 0.066257 TPR: 0.376667. FPR: 0.248047\n",
            "MLP: 3. F1: 0.144568. +- 0.052081 MCC: 0.107704. +- 0.056444 TPR: 0.176667. FPR: 0.049089\n",
            "TRAIN TIME:\n",
            "1.1s\n",
            "Alpha: 0.40. Beta: 0.00. IR_sw: 1.36.\n",
            "MLP: 1. F1: 0.083588. +- 0.062643 MCC: 0.038784. +- 0.069329 TPR: 0.123333. FPR: 0.070443\n",
            "MLP: 3. F1: 0.137054. +- 0.078103 MCC: 0.102862. +- 0.081655 TPR: 0.146667. FPR: 0.038672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-B7-Zl_RQz-",
        "outputId": "267e0680-e62c-406d-efaf-e9f49e8b48c9"
      },
      "source": [
        "MLP_ens[1].fit(X_train_s, y_train_s)\n",
        "print(MLP_ens[1].predict_proba(X_test_n)[:,1])\n",
        "# print(o_x_tst)\n",
        "print(gamma)\n",
        "print(y_true)\n",
        "print(0.5*(y_true+1)*MLP_ens[1].predict_proba(X_test_n)[:,1])\n",
        "print(0.5*(y_true+1)*o_x_tst_ens[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.33333333 0.         0.33333333 0.33333333 0.33333333 0.66666667\n",
            " 0.         0.33333333 1.         0.66666667 0.66666667 0.33333333\n",
            " 0.         0.66666667 0.66666667 1.         0.33333333 0.33333333\n",
            " 0.66666667 0.33333333 0.33333333 0.33333333 0.66666667 0.33333333\n",
            " 0.33333333 0.33333333 0.33333333 1.         0.33333333 1.\n",
            " 0.33333333 1.         0.         0.         0.33333333 0.\n",
            " 0.         0.33333333 0.66666667 0.66666667 0.         0.33333333\n",
            " 0.66666667 0.33333333 0.33333333 0.         0.         1.\n",
            " 0.         0.66666667 1.         0.33333333 0.33333333 0.\n",
            " 0.33333333 0.33333333 1.         0.66666667 0.33333333 0.\n",
            " 0.33333333 0.66666667 0.33333333 0.66666667 0.33333333 0.33333333\n",
            " 0.66666667 0.         0.         0.         0.66666667 0.33333333\n",
            " 0.66666667 0.         0.         0.33333333 0.         0.33333333\n",
            " 0.33333333 0.33333333 0.66666667 0.         0.         0.33333333\n",
            " 0.66666667 0.66666667 0.66666667 0.         0.66666667 0.66666667\n",
            " 0.66666667 0.33333333 0.33333333 0.66666667 0.66666667 0.33333333\n",
            " 0.33333333 0.33333333 0.33333333 0.         0.         0.33333333\n",
            " 0.33333333 0.33333333 0.33333333 0.         1.         0.33333333\n",
            " 0.33333333 0.33333333 0.33333333 0.33333333 0.         1.\n",
            " 0.66666667 0.         0.         0.33333333 0.66666667 0.33333333\n",
            " 0.66666667 0.33333333 1.         0.         0.33333333 0.\n",
            " 0.66666667 0.33333333 0.33333333 0.         0.33333333 0.\n",
            " 0.         0.66666667 0.33333333 0.33333333 0.66666667 0.\n",
            " 0.66666667 0.33333333 0.         0.66666667 0.         0.66666667\n",
            " 0.         0.         0.66666667 0.66666667 0.33333333 0.\n",
            " 0.33333333 0.66666667 1.         0.         0.33333333 0.33333333\n",
            " 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.66666667\n",
            " 0.33333333 0.33333333 0.         0.66666667 0.         0.66666667\n",
            " 0.33333333 0.66666667 0.33333333 0.66666667 0.33333333 0.33333333\n",
            " 0.66666667 0.33333333 0.         0.33333333 0.         0.\n",
            " 0.66666667 0.66666667 0.66666667 0.66666667 0.         0.33333333\n",
            " 0.33333333 0.         0.66666667 0.66666667 0.66666667 0.66666667\n",
            " 1.         0.33333333 0.66666667 0.33333333 1.         0.\n",
            " 0.66666667 0.         0.66666667 0.33333333 0.66666667 0.33333333\n",
            " 0.33333333 0.         0.66666667 0.66666667 0.33333333 0.66666667\n",
            " 0.         0.33333333 0.         0.         0.         0.66666667\n",
            " 0.33333333 0.66666667 0.66666667 0.66666667 0.33333333 0.\n",
            " 0.         0.33333333 0.33333333 0.33333333 0.         0.33333333\n",
            " 0.33333333 0.66666667 0.33333333 0.33333333 0.33333333 0.66666667\n",
            " 0.66666667 0.33333333 0.33333333 0.         0.66666667 0.66666667\n",
            " 0.         0.33333333 0.66666667 0.33333333 0.66666667 0.33333333\n",
            " 0.66666667 1.         0.66666667 0.33333333 0.         0.33333333\n",
            " 0.         0.         0.         0.33333333 0.66666667 0.\n",
            " 0.         0.         0.         0.         0.66666667 0.\n",
            " 0.33333333 0.         0.66666667 0.33333333 0.66666667 0.\n",
            " 0.66666667 0.66666667 0.         0.66666667 0.66666667 0.66666667\n",
            " 0.33333333 0.33333333 1.         0.33333333 0.33333333 0.\n",
            " 0.33333333 1.         0.         0.66666667 0.33333333 1.\n",
            " 0.         0.33333333 0.         0.         0.         0.66666667\n",
            " 0.33333333 0.33333333 0.         0.33333333 0.66666667 0.33333333\n",
            " 0.33333333 0.33333333 0.33333333 0.66666667 0.         0.33333333\n",
            " 0.         0.66666667 0.66666667 0.33333333 0.33333333 0.\n",
            " 0.         0.66666667 1.         0.         0.         0.33333333\n",
            " 0.         0.33333333 0.33333333 0.66666667 0.66666667 0.66666667\n",
            " 0.66666667 1.         1.         0.33333333 0.         0.33333333\n",
            " 0.33333333 0.33333333 0.33333333 1.         0.         0.66666667\n",
            " 0.         0.33333333 0.33333333 0.         0.66666667 0.\n",
            " 0.66666667 0.66666667 0.33333333 0.33333333 0.         0.33333333\n",
            " 0.         0.66666667 0.         1.         0.66666667 0.33333333\n",
            " 0.         1.         0.33333333 1.         0.66666667 0.66666667\n",
            " 0.         0.33333333 0.         0.33333333 0.66666667 0.66666667\n",
            " 0.         0.33333333 0.66666667 0.         0.33333333 0.\n",
            " 0.         0.66666667 0.33333333 0.66666667 0.33333333 1.\n",
            " 0.33333333 0.         0.66666667 0.33333333 0.33333333 0.33333333\n",
            " 0.33333333 0.66666667 0.33333333 0.33333333 0.33333333 0.66666667\n",
            " 0.66666667 0.33333333 0.66666667 0.33333333 0.66666667 0.33333333\n",
            " 0.33333333 0.33333333 0.        ]\n",
            "0.3999999999999999\n",
            "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         1.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         1.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.33333333 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         1.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.66666667 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.66666667 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.66666667\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.66666667 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.33333333 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.33333333\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.66666667\n",
            " 0.         0.         1.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.33333333 0.         0.         0.         0.66666667\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.        ]\n",
            "[-0.         -0.         -0.          0.         -0.          0.\n",
            " -0.          0.          0.         -0.          0.          0.\n",
            " -0.         -0.         -0.         -1.         -0.         -0.\n",
            " -0.          0.         -0.          0.         -0.         -0.\n",
            " -0.         -0.         -0.          0.          0.          0.\n",
            "  0.          0.         -0.          0.         -0.          0.\n",
            "  0.         -0.         -0.         -0.         -0.          0.\n",
            " -0.          0.         -0.          0.         -0.         -0.\n",
            " -0.         -0.          1.         -0.         -0.          0.\n",
            " -0.          0.         -0.         -0.          0.         -0.\n",
            "  0.33333333 -0.         -0.         -0.         -0.          0.\n",
            " -0.         -0.         -0.33333333 -0.         -0.         -0.\n",
            " -0.         -0.          0.         -0.          0.         -0.\n",
            " -0.         -0.         -0.         -0.         -0.         -0.\n",
            " -0.         -0.         -0.         -0.          0.         -0.\n",
            "  0.          0.          0.          0.         -0.         -0.\n",
            " -0.          0.         -0.         -0.          0.         -0.\n",
            "  0.         -0.          0.          0.          0.          0.\n",
            "  0.          0.         -0.         -0.          0.         -0.\n",
            " -0.          0.          0.          0.         -0.          0.\n",
            " -0.         -0.         -1.          0.          0.          0.\n",
            " -0.          0.         -0.         -0.          0.         -0.\n",
            " -0.         -0.         -0.          0.         -0.33333333  0.\n",
            " -0.         -0.         -0.         -0.         -0.         -0.\n",
            " -0.         -0.         -0.          1.          0.          0.\n",
            " -0.          0.         -0.          0.         -0.          0.\n",
            "  0.          0.         -0.         -0.          0.          0.\n",
            "  0.          0.          0.         -0.         -0.         -0.\n",
            " -0.          0.         -0.          0.          0.         -0.\n",
            "  0.         -0.         -0.          0.          0.          0.\n",
            "  0.         -0.          0.         -0.         -0.         -0.\n",
            " -0.         -0.         -0.         -0.          0.         -0.\n",
            "  0.          0.         -0.          0.          0.          0.\n",
            " -0.          0.         -0.         -0.         -0.         -0.\n",
            "  0.          0.         -0.         -0.          0.          0.33333333\n",
            "  0.         -0.         -0.          0.         -0.         -0.\n",
            "  0.         -0.          0.33333333 -0.          0.         -0.\n",
            " -0.          0.         -0.         -0.         -0.         -0.\n",
            "  0.         -0.          0.          0.         -0.         -0.\n",
            " -0.          0.         -0.         -0.          0.          0.\n",
            " -0.         -0.         -0.         -0.          0.         -0.\n",
            " -0.         -0.         -0.         -0.         -0.         -0.\n",
            "  0.         -0.         -0.         -0.          0.         -0.\n",
            "  0.          0.          0.          0.         -0.          0.\n",
            "  0.          0.         -0.         -0.          0.          0.\n",
            "  0.         -0.         -0.         -0.         -0.         -0.\n",
            " -0.         -0.          0.          0.          0.          0.\n",
            "  0.         -0.         -0.         -0.          0.          0.\n",
            "  0.          0.         -0.          0.          0.         -0.\n",
            " -0.33333333 -0.          0.         -0.         -0.          0.\n",
            "  0.         -0.         -0.         -0.         -0.          0.33333333\n",
            "  0.         -0.         -0.         -0.         -0.          0.\n",
            "  0.         -0.         -0.          0.          0.          0.\n",
            " -0.          0.          0.          0.          0.          0.33333333\n",
            " -0.         -0.          1.         -0.         -0.         -0.\n",
            " -0.         -0.         -0.         -0.         -0.          0.\n",
            "  0.          0.         -0.          0.         -0.         -0.\n",
            "  0.         -0.         -0.         -0.         -0.         -0.\n",
            "  0.         -0.         -0.          0.         -0.         -0.\n",
            " -0.         -0.         -0.         -0.         -0.          0.\n",
            " -0.         -0.33333333 -0.          0.          0.          0.33333333\n",
            "  0.         -0.         -0.         -0.         -0.         -0.\n",
            " -0.          0.         -0.         -0.         -0.          0.\n",
            " -0.         -0.         -0.         -0.         -0.          0.\n",
            "  0.         -0.          0.         -0.          0.         -0.\n",
            "  0.         -0.         -0.         -0.          0.          0.\n",
            "  0.         -0.         -0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}